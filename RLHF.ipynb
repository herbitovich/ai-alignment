{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44f54cb-221c-4d4a-abfd-f9ac1b1b5454",
   "metadata": {},
   "source": [
    "# AI Alignment with the REINFORCE algorithm\n",
    "Вместо принятого для метода алаймента RLHF использования вычислительно тяжёлого алгоритма Proximal Policy Optimization, считающего генерацию каждого токена моделью за отдельное действие, реализуем алгоритм REINFORCE, воспринимающий за действия агента уже сгенерированные последовательности.\n",
    "## Level 1\n",
    "Реализуем REINFORCE w/ baseline, равным moving average.<br>\n",
    "<b>Сетап.</b> В качестве RewardModel будем использовать ту же модель, что и для самого процесса алаймента $-$ SmolLM2-Instruct на 135M параметров.<br><br>\n",
    "<b>Датасет.</b> В использующемся [датасете](https://huggingface.co/datasets/esfrankel17/HelpSteer2_binarized) представлены следующие столбцы:<br>\n",
    "\n",
    "| Prompt | Chosen | Chosen Rating | Rejected | Rejected Rating |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "\n",
    "Согласно документации RewardTrainer, [ссылающейся](https://huggingface.co/docs/trl/main/en/reward_trainer#adding-a-margin-to-the-loss) на Llama 2.1 paper, `Chosen Rating` и `Rejected Rating` можно использовать для вычисления столбца `margin`, который затем учитывается в лоссе RewardModel. Откажемся от этой практики, т.к. она больше не является state-of-the-art стандартом <i>(Llama 3.1 paper, section 4.1.2)</i>.\n",
    "<br><br>\n",
    "<b>Приступаем к коду.</b> Импортируем нужные библиотеки, инициализируем модели, загружаем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250a645-d9bb-4f38-9afc-3edca4109dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, use_fast=False)\n",
    "\n",
    "dataset = load_dataset(\"esfrankel17/HelpSteer2_binarized\")['average_rating_split']\n",
    "\n",
    "dataset = dataset.remove_columns([\"chosen_rating\", \"rejected_rating\"]) # Not SOTA anymore to use 'margin' (Llama 3.1 paper)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940e179-c44d-4706-a879-226d0f2bfbec",
   "metadata": {},
   "source": [
    "Разделим датасет на тренировочную и валидационную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84f1d2-d34b-4398-9b0d-0e647601e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23213df3-4c3f-4db3-801f-edc0e90c317c",
   "metadata": {},
   "source": [
    "Начнем тренировку RewardModel с помощью RewardTrainer. Не забудем указать рекомендованное значение для параметра `center_rewards_coefficient`, т.к. хотим, чтобы средний аутпут модели награды был равен 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cb68c-a2e8-48e5-9da1-24b880a766a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_config = RewardConfig(\n",
    "    report_to=\"none\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_length=256,\n",
    "    center_rewards_coefficient = 0.01, # Recommended, as it is preferred that the reward's model output is mean zero.\n",
    "    output_dir=\"./reward_model\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset.remove_columns(\"prompt\"),\n",
    "    eval_dataset=val_dataset.remove_columns(\"prompt\"),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6df18-ff13-44ee-9cbd-e04a7a810966",
   "metadata": {},
   "source": [
    "Реализуем сам REINFORCE. Формула из оригинальной статьи выглядит следующим образом:<br>\n",
    "$ E_ {x\\sim D,y\\sim \\pi_\\theta (x)} [(R(y,x)-b) \\nabla _ {\\theta } $ $ \\log _ {\\pi_\\theta }  (y|x)]$<br>\n",
    "Иными словами, для каждого промпта $x$ мы хотим засэмплировать (согласно вероятностному распределению политики модели $\\pi_\\theta$) некую респонс-последовательность $y$. Для каждой такой пары для вычисления лосса нам понадобится знать награду $R(x,y)$, вычисленной для данной пары нашей RewardModel, а так же вероятность именно этого $y$ в изначальном распределении. В качестве baseline используем moving average, то есть $b_{MA} = \\frac{1}{S} \\sum_s R(x^s, y^s)$.<br>\n",
    "Ключевые моменты процесса тренировки:<br>\n",
    "* Для оптимизации везде работаем с батчами данных\n",
    "* Для получения в меру возможностей разумных ответов, каждый раз, передавая промпт в модельку, применяем шаблон чата\n",
    "* Для вычисления вероятности сэмплирования $y$ - маскируем промпты, для вычисления награды $R(x,y)$ - нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eff3dd-1ebe-4e0e-9f89-751827282b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "#reward_model = AutoModelForSequenceClassification.from_pretrained(\"checkpoint\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, padding_side=\"left\")\n",
    "optimizer = optim.Adam(sft_model.parameters(), lr=0.01)\n",
    "num_epochs = 1\n",
    "max_length = 50\n",
    "\n",
    "def get_reward(reward_model, tokenizer, responses):\n",
    "    rewards = []\n",
    "    for response in responses:\n",
    "        inputs = tokenizer.encode_plus(response, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = reward_model(**inputs)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        rewards.append(logits.item())\n",
    "    return rewards\n",
    "\n",
    "def evaluate():\n",
    "    all_rewards = []\n",
    "    for batch in val_dataset.iter(batch_size=16):\n",
    "        prompts = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False) for prompt in batch[\"prompt\"]]\n",
    "        tokenized_prompts = tokenizer.batch_encode_plus(prompts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = sft_model.generate(input_ids=tokenized_prompts[\"input_ids\"], attention_mask=tokenized_prompts[\"attention_mask\"], max_new_tokens=max_length)\n",
    "        responses = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        rewards = get_reward(reward_model, tokenizer, responses)\n",
    "        print(\"New batch!\")\n",
    "        all_rewards.extend(rewards)\n",
    "    print(f\"Mean reward on the validation dataset: {sum(all_rewards) / len(all_rewards)}\")\n",
    "\n",
    "def train():\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        all_rewards = []\n",
    "        for batch in train_dataset.iter(batch_size=16):\n",
    "\n",
    "            print(\"New batch!\")\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Tokenize prompts\n",
    "            prompts = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False) for prompt in batch[\"prompt\"]]\n",
    "            tokenized_prompts = tokenizer.batch_encode_plus(\n",
    "                prompts, \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=max_length, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(sft_model.device)\n",
    "\n",
    "            # Generate responses without tracking gradients\n",
    "            with torch.no_grad():\n",
    "                generated_sequences = sft_model.generate(\n",
    "                    input_ids=tokenized_prompts['input_ids'],\n",
    "                    attention_mask=tokenized_prompts['attention_mask'],\n",
    "                    max_new_tokens=max_length,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            print(\"Generated.\")\n",
    "            # Compute log probabilities for generated tokens\n",
    "            outputs = sft_model(generated_sequences, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate log probs for generated tokens (excluding prompt)\n",
    "            shift_logits = logits[:, :-1, :]  # Skip last token logits\n",
    "            shift_labels = generated_sequences[:, 1:]  # Skip first token (prompt start)\n",
    "            \n",
    "            # Create mask to ignore prompt tokens\n",
    "            prompt_lengths = tokenized_prompts['attention_mask'].sum(dim=1)\n",
    "            mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
    "            for i, length in enumerate(prompt_lengths):\n",
    "                mask[i, length-1:] = True  # Start masking from end of prompt\n",
    "                \n",
    "            # Compute log probabilities\n",
    "            log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "            selected_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "            selected_log_probs = selected_log_probs * mask  # Zero out prompt tokens\n",
    "            total_log_probs = selected_log_probs.sum(dim=1)\n",
    "\n",
    "            responses = tokenizer.batch_decode(generated_sequences, skip_special_tokens=False)\n",
    "            rewards = torch.tensor(get_reward(reward_model, tokenizer, responses)).float().to(sft_model.device)\n",
    "            \n",
    "            \n",
    "            baseline = np.mean(all_rewards) if all_rewards else 0\n",
    "            rewards = rewards - baseline\n",
    "            all_rewards.append(rewards.mean().item()) # We can do that as 'mean' is an associative operation\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = (-total_log_probs * rewards).sum()\n",
    "            \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Updated the weights\")\n",
    "\n",
    "evaluate()\n",
    "train()\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bbeae-d38d-4340-815c-28241e1b13eb",
   "metadata": {},
   "source": [
    "По итогу тренировки получили весьма значительное для одной эпохи улучшение:\n",
    "\n",
    "| Mean reward pre-RLHF | Mean reward post-RLHF |\n",
    "| :-: | :-: |\n",
    "| -1.59 | -1.11 |\n",
    "\n",
    "Сохраняем модель, радуемся жизни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e586af-3e05-4c21-aab1-9944146055d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model.save_pretrained(\"sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19304cbf-251f-41ce-bfa9-115d258405fa",
   "metadata": {},
   "source": [
    "## Level 2\n",
    "Обучим Reward Model на выдачу вероятностного распределения.<br>\n",
    "Какую информацию мы можем найти в вероятностном распределении оценок? Очевидно, основа такой оценки награды - матожидание вероятностного распределения. С другой стороны, если мы будем опираться лишь на матожидание, то такой подход, во-первых, будет мало отличаться от скалярного значения аутпута модели награды из Level 1, а во-вторых, является очень грубым: когда модель награды не понимает, что сказать, и выдаёт выборку с вероятностью $0,5$ у оценки \"$1$\" и с вероятностью $0,5$ у оценки \"$10$\", вряд ли мы захотим, чтобы веса LM оставались примерно такими же. Поэтому для вычисления лосса будем также учитывать дисперсию распределения. Итоговая формула будет выглядеть примерно следующим образом:<br>\n",
    "$ E_ {x\\sim D,y\\sim \\pi_\\theta (x), r \\sim p(r|y,x)} [(E_[r]-b-\\lambda Var(r)) \\nabla _ {\\theta } $ $ \\log _ {\\pi_\\theta }  (y|x)]$<br>\n",
    "Где $\\lambda$ - коэффицент того, насколько дисперсия влияет на лосс.<br><br>\n",
    "<b>Датасет.</b> Преобработаем датасет, превратив `chosen_rating` и `rejected_rating` в вероятностные распределения по следюущему правилу: $2,5$ - середина отрезка $[2, 3]$, превратим в распределение с вероятностью $0,5$ у оценки \"$2$\" и $0,5$ у оценки \"$3$\". В исходном датасете значения этих полей находятся на отрезке $[0, 4]$, построим отображение $f : [0, 4] \\rightarrow [1, 10]$, используя формулу $\\frac{(x-a)(d-c)}{(b-a)}+c$.<br> $f(x) = \\frac{9}{4}x + 1$.<br><br>\n",
    "<b>Приступаем к коду.</b> Начальный этап подготовки и тренировки модели наград остался неизменным с Level 1, за исключением функции предобработки данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9603902-303a-4739-ba4e-7d150c127d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "reward_distribution_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, use_fast=False)\n",
    "\n",
    "dataset = load_dataset(\"esfrankel17/HelpSteer2_binarized\")['average_rating_split']\n",
    "\n",
    "def translate(x : float):\n",
    "    return 2.25*x + 1\n",
    "\n",
    "def to_distribution(reward : float):\n",
    "    reward = translate(reward)\n",
    "    distribution = np.zeros(10)\n",
    "\n",
    "    rounded = int(reward)\n",
    "    prob_bigger = reward - rounded\n",
    "    prob_smaller = 1 - prob_bigger\n",
    "\n",
    "    distribution[rounded-1] = prob_smaller\n",
    "    if rounded<=9:\n",
    "        distribution[rounded] = prob_bigger\n",
    "\n",
    "    return distribution\n",
    "    \n",
    "def tokenize(text, max_length):\n",
    "    text = tokenizer.apply_chat_template(text, tokenize=False)\n",
    "    tokenized_text = tokenizer.encode_plus(\n",
    "        text, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokenized_text\n",
    "    \n",
    "def preprocess(examples):\n",
    "    chosen_tokenized = tokenize(examples[\"chosen\"], 256)\n",
    "    rejected_tokenized = tokenize(examples[\"rejected\"], 256)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_tokenized[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask_chosen\": chosen_tokenized[\"attention_mask\"].squeeze(0),\n",
    "        \"input_ids_rejected\": rejected_tokenized[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask_rejected\": rejected_tokenized[\"attention_mask\"].squeeze(0),\n",
    "        \"chosen_labels\": to_distribution(examples[\"chosen_rating\"]),\n",
    "        \"rejected_labels\": to_distribution(examples[\"rejected_rating\"]),\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903598f-64da-487c-8f28-2a4263701793",
   "metadata": {},
   "source": [
    "Разбиваем датасет на тренировочную/валидационную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e3115-393c-442d-aac3-2c3ddd0c5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69244acf-7688-4dbc-9de5-cd2b78ecdab9",
   "metadata": {},
   "source": [
    "Начинаем тренировку RewardModel.<br>\n",
    "Очевидно, дефолтный лосс нам тут не подойдет, так как, во-первых, классов у нас не один, а десять (награды 1-10), а во-вторых, работаем мы с логитами. Переопределим RewardTrainer и дефолтный `data_collator`, чтобы в сам лосс к нам обязательно попадали все столбцы, в т.ч. неиспользуемые в forward (в этом конкретном случае - `chosen_labels`, `rejected_labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2d09c-c397-44ca-bd5c-a031198aa5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardTrainer, RewardConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class RewardDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer, padding=True)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Prepare the inputs for padding\n",
    "        chosen_features = [{\"input_ids\": f[\"input_ids_chosen\"], \"attention_mask\": f[\"attention_mask_chosen\"]} for f in features]\n",
    "        rejected_features = [{\"input_ids\": f[\"input_ids_rejected\"], \"attention_mask\": f[\"attention_mask_rejected\"]} for f in features]\n",
    "\n",
    "        # Use the parent class's __call__ method to pad inputs\n",
    "        chosen_batch = super().__call__(chosen_features)\n",
    "        rejected_batch = super().__call__(rejected_features)\n",
    "\n",
    "        # Include labels\n",
    "        batch = {\n",
    "            \"input_ids_chosen\": chosen_batch[\"input_ids\"],\n",
    "            \"attention_mask_chosen\": chosen_batch[\"attention_mask\"],\n",
    "            \"input_ids_rejected\": rejected_batch[\"input_ids\"],\n",
    "            \"attention_mask_rejected\": rejected_batch[\"attention_mask\"],\n",
    "            \"chosen_labels\": torch.tensor([f[\"chosen_labels\"] for f in features], dtype=torch.float),\n",
    "            \"rejected_labels\": torch.tensor([f[\"rejected_labels\"] for f in features], dtype=torch.float),\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "class DistributionRewardTrainer(RewardTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        #print(inputs[\"input_ids_chosen\"])\n",
    "        # Forward pass for CHOSEN responses\n",
    "        #print(inputs[\"input_ids_chosen\"].shape)\n",
    "        chosen_outputs = model(\n",
    "            input_ids=inputs[\"input_ids_chosen\"],\n",
    "            attention_mask=inputs[\"attention_mask_chosen\"],\n",
    "        )\n",
    "        # Log probabilities for chosen\n",
    "        chosen_log_probs = torch.log_softmax(chosen_outputs.logits, dim=-1)\n",
    "        # KL divergence: compares predicted (log_probs) vs target (chosen_labels)\n",
    "        chosen_loss = torch.nn.functional.kl_div(\n",
    "            chosen_log_probs,\n",
    "            inputs[\"chosen_labels\"],\n",
    "            reduction=\"batchmean\",     # Average loss over the batch\n",
    "        )\n",
    "\n",
    "        # Forward pass for REJECTED responses\n",
    "        rejected_outputs = model(\n",
    "            input_ids=inputs[\"input_ids_rejected\"],\n",
    "            attention_mask=inputs[\"attention_mask_rejected\"],\n",
    "        )\n",
    "        rejected_log_probs = torch.log_softmax(rejected_outputs.logits, dim=-1)\n",
    "        rejected_loss = torch.nn.functional.kl_div(\n",
    "            rejected_log_probs,\n",
    "            inputs[\"rejected_labels\"],\n",
    "            reduction=\"batchmean\",\n",
    "        )\n",
    "\n",
    "        total_loss = chosen_loss + rejected_loss\n",
    "\n",
    "        if return_outputs:\n",
    "            return total_loss, {\n",
    "                \"chosen_outputs\": chosen_outputs,\n",
    "                \"rejected_outputs\": rejected_outputs\n",
    "            }\n",
    "        return total_loss\n",
    "\n",
    "data_collator = RewardDataCollator(tokenizer)\n",
    "\n",
    "reward_config = RewardConfig(\n",
    "    report_to=\"none\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_length=256,\n",
    "    center_rewards_coefficient = 0.01, # Recommended, as it is preferred that the reward's model output is mean zero.\n",
    "    output_dir=\"./reward_model_distribution\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = DistributionRewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e7ccf-9c80-40c1-bc25-fe77114d9c80",
   "metadata": {},
   "source": [
    "Реализуем выведенный раннее подход к вычислению лосса для \"распределительной\" модели наград. Функцию `evaluate` оставляем старую для возможности сравнения результатов между двумя моделями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16448c-0b7d-4761-bce6-70e00adca6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "#reward_model = AutoModelForSequenceClassification.from_pretrained(\"checkpoint\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, padding_side=\"left\")\n",
    "optimizer = optim.Adam(sft_model.parameters(), lr=0.01)\n",
    "num_epochs = 1\n",
    "max_length = 50\n",
    "\n",
    "def get_reward_distribution(reward_model, tokenizer, responses):\n",
    "    rewards = []\n",
    "    for response in responses:\n",
    "        inputs = tokenizer.encode_plus(response, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = reward_model(**inputs)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        log_probs = torch.softmax(logits, dim=-1).squeeze(0).tolist()\n",
    "        rewards.append(log_probs)\n",
    "    return rewards\n",
    "\n",
    "def train():\n",
    "    lambda_coeff = 0.1 # Check the formula above\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        all_rewards = []\n",
    "        for batch in train_dataset.iter(batch_size=16):\n",
    "\n",
    "            print(\"New batch!\")\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Tokenize prompts\n",
    "            prompts = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False) for prompt in batch[\"prompt\"]]\n",
    "            tokenized_prompts = tokenizer.batch_encode_plus(\n",
    "                prompts, \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=max_length, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(sft_model.device)\n",
    "\n",
    "            # Generate responses without tracking gradients\n",
    "            with torch.no_grad():\n",
    "                generated_sequences = sft_model.generate(\n",
    "                    input_ids=tokenized_prompts['input_ids'],\n",
    "                    attention_mask=tokenized_prompts['attention_mask'],\n",
    "                    max_new_tokens=max_length,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            print(\"Generated.\")\n",
    "            # Compute log probabilities for generated tokens\n",
    "            outputs = sft_model(generated_sequences, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate log probs for generated tokens (excluding prompt)\n",
    "            shift_logits = logits[:, :-1, :]  # Skip last token logits\n",
    "            shift_labels = generated_sequences[:, 1:]  # Skip first token (prompt start)\n",
    "            \n",
    "            # Create mask to ignore prompt tokens\n",
    "            prompt_lengths = tokenized_prompts['attention_mask'].sum(dim=1)\n",
    "            mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
    "            for i, length in enumerate(prompt_lengths):\n",
    "                mask[i, length-1:] = True  # Start masking from end of prompt\n",
    "                \n",
    "            # Compute log probabilities\n",
    "            log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "            selected_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "            selected_log_probs = selected_log_probs * mask  # Zero out prompt tokens\n",
    "            total_log_probs = selected_log_probs.sum(dim=1)\n",
    "\n",
    "            responses = tokenizer.batch_decode(generated_sequences, skip_special_tokens=False)\n",
    "\n",
    "            baseline = np.mean(all_rewards) if all_rewards else 0\n",
    "            \n",
    "            rewards = get_reward_distribution(reward_distribution_model, tokenizer, responses)\n",
    "            \n",
    "            expected_values = [sum([distr[x]*(x+1) for x in range(10)]) for distr in rewards]\n",
    "            expected_values_sq = [sum([distr[x]*(x+1)**2 for x in range(10)]) for distr in rewards]\n",
    "            most_probable = [np.argmax(distr)+1 for distr in rewards]\n",
    "            var = [expected_values_sq[i] - expected_values[i]**2 for i in range(len(rewards))]\n",
    "            \n",
    "            expected_with_variance = [expected_values[i] - baseline - var[i]*lambda_coeff]\n",
    "            \n",
    "            all_rewards.extend(most_probable)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = (-total_log_probs * np.sum(expected_with_variance)).sum()\n",
    "            \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Updated the weights\")\n",
    "\n",
    "evaluate()\n",
    "train()\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a6f8b-d18e-4e9d-b30f-208d3adf6d6e",
   "metadata": {},
   "source": [
    "Т.к. функция `evaluate` работает с предыдущей RewardModel, выдающей скалярные значения, можем получить более-менее сравнимые данные:\n",
    "| Mean reward pre-RLHF | Mean reward post-RLHF |\n",
    "| :-: | :-: |\n",
    "| -2.02 | -1.23 |\n",
    "\n",
    "Что видим в итоге? REINFORCE со \"скалярной\" RewardModel улучшил значение средней награды на валидационном датасете на ~30%, REINFORCE с \"распределительной\" - на ~40%. В сравнении абсолютных значений последняя модель тоже отрывается.\n",
    "\n",
    "## Выводы\n",
    "Почему так получилось? Думаю, что своим отрывом \"распределительная\" RewardModel в алгоритме REINFORCE обязана дополнительному параметру \"неуверенности\" модели - дисперсии. Мы не имели доступа к таким данным со \"скалярной\" моделью награды; как выяснилось, достаточно часто модель мечется между несколькими, казалось бы, противоположными классами-оценками награды. \n",
    "### Что крутого узналось?\n",
    "* Изначально я не планировал использовать chat template на промпте/респонсе модели, но, на удивление, это ОЧЕНЬ сильно повысило \"разумность\" ответов модели. Конечно, в ходе тренировки модель, в токенизированном входе/выходе которой не учавствует шаблон чата, рано или поздно нашла бы способ найти некий минимум функции лосса, \"обманывая\" модель награды; но что это тогда за RLHF? Так что несмотря на несколько усложненный процесс токенизации, оно того однозначно стоило.\n",
    "* Много времени кодинга было убито на то, чтобы разрешить следующую проблему: первоначально, в коде для получения вероятности сэмплирования последовательности $y$ был использован многократный `model(**inputs)` (для каждого нового токена) с вычислением градиента. Это более интуитивный подход, вычисляющий вероятность сэмплирования каждого токена и затем уже всей последовательности. Но, как оказалось, в слегка контритуитивном переходе на `with torch.no_grad(): model.generate(**inputs)` (то есть генерации сразу всей последовательности и вычисления вероятности ее сэмплирования пост-фактум) кроилось решение оптимизации вычислений во много-много раз.\n",
    "### Что не получилось реализовать?\n",
    "В силу временных и вычислительных ограничений у меня не вышло сохранить один валидационный датасет на обе модели награды, \"скалярную\" и \"распределительную\"; более того, последняя обучилась на слегка мЕньшем срезе данных из датасета. Оба фактора могут повлиять на воспроизводимость полученных результатов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf_env",
   "language": "python",
   "name": "rlhf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
